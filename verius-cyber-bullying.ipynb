{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish Cyber Bullying \n",
    "\n",
    "The data I have used : https://www.kaggle.com/abozyigit/turkish-cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sk-learn\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set()\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seaborn: 0.9.0\n",
      "Pandas 0.22.0\n",
      "Numpy 1.13.3\n",
      "Matplotlib 2.2.2\n",
      "Sclearn 0.20.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Seaborn:\", sns.__version__)\n",
    "print(\"Pandas\", pd.__version__)\n",
    "print(\"Numpy\", np.__version__)\n",
    "print(\"Matplotlib\", matplotlib.__version__)\n",
    "print(\"Sclearn\", sklearn.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>cyberbullying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rabbim kalan ömrünü geçen ömründen hayırlı eylesin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bir ateist olarak bu resmi gördükçe gözyaşlarıma mani olamıyorum</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oo süpersin azıcık bize de bulaşsa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bende biliyorum benden bı bok olmicak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nerdesin len tirrek</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            message  \\\n",
       "0                rabbim kalan ömrünü geçen ömründen hayırlı eylesin   \n",
       "1  bir ateist olarak bu resmi gördükçe gözyaşlarıma mani olamıyorum   \n",
       "2                                oo süpersin azıcık bize de bulaşsa   \n",
       "3                             bende biliyorum benden bı bok olmicak   \n",
       "4                                               nerdesin len tirrek   \n",
       "\n",
       "   cyberbullying  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              1  \n",
       "4              1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/turkish-cyber-bullying.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3001 entries, 0 to 3000\n",
      "Data columns (total 2 columns):\n",
      "message          3001 non-null object\n",
      "cyberbullying    3001 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples for binary classification: [1498 1503]\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples for binary classification: {}\".format(np.bincount(df.cyberbullying)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1498_ 0s and _1503_ 1s in the cyberbullying column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(df, df.cyberbullying):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500832\n",
       "0    0.499168\n",
       "Name: cyberbullying, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set.cyberbullying.value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500833\n",
       "0    0.499167\n",
       "Name: cyberbullying, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set.cyberbullying.value_counts() / len(strat_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cyber = cyber.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cyber.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "vect.fit(x_cyber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of vocabularies: 9687\n",
      "Vocabulary 'affet' occurs : 178 times.\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of vocabularies: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary 'affet' occurs : {} times.\".format(vect.vocabulary_['affet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<2400x9687 sparse matrix of type '<class 'numpy.int64'>'\\n\\twith 21995 stored elements in Compressed Sparse Row format>\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = vect.transform(x_cyber)\n",
    "repr(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 vocabularies:\n",
      "['09052018', '0islam', '10', '100', '100m', '1085', '10yıl', '11', '110', '12', '1200', '13', '14', '15', '1600', '165', '18', '1957', '1975', '1takımdan']\n",
      "Vocabularies 3010 to 3030:\n",
      "['flamingo', 'flamingolar', 'flamingolara', 'flamingoları', 'flamingoların', 'flood', 'floryadan', 'flört', 'fondotensiz', 'football', 'forma', 'formasına', 'formasıyla', 'format', 'forumlar', 'forvet', 'foseptikten', 'foto', 'fotolarım', 'fotolarını']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "print(\"First 20 vocabularies:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Vocabularies 3010 to 3030:\\n{}\".format(feature_names[3010:3030]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer `min_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5)\n",
    "\n",
    "vect.fit(x_cyber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<2400x649 sparse matrix of type '<class 'numpy.int64'>'\\n\\twith 10378 stored elements in Compressed Sparse Row format>\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = vect.transform(x_cyber)\n",
    "repr(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 vocabularies:\n",
      "['09052018', '0islam', '10', '100', '100m', '1085', '10yıl', '11', '110', '12', '1200', '13', '14', '15', '1600', '165', '18', '1957', '1975', '1takımdan']\n",
      "Vocabularies 3010 to 3030:\n",
      "['flamingo', 'flamingolar', 'flamingolara', 'flamingoları', 'flamingoların', 'flood', 'floryadan', 'flört', 'fondotensiz', 'football', 'forma', 'formasına', 'formasıyla', 'format', 'forumlar', 'forvet', 'foseptikten', 'foto', 'fotolarım', 'fotolarını']\n"
     ]
    }
   ],
   "source": [
    "eature_names = vect.get_feature_names()\n",
    "\n",
    "print(\"First 20 vocabularies:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Vocabularies 3010 to 3030:\\n{}\".format(feature_names[3010:3030]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer `stop_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"turkish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey', 'birşey', 'birkaç', 'nerde', 'nasıl', 'neden', 'sanki', 'gibi', 'ama', 'kim', 'defa', 'de', 'eğer', 'mu', 'her', 'nerede', 'o', 'nereye', 'mü'],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words=list(stops))\n",
    "\n",
    "vect.fit(x_cyber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<2400x607 sparse matrix of type '<class 'numpy.int64'>'\\n\\twith 8020 stored elements in Compressed Sparse Row format>\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = vect.transform(x_cyber)\n",
    "repr(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 vocabularies:\n",
      "['09052018', '0islam', '10', '100', '100m', '1085', '10yıl', '11', '110', '12', '1200', '13', '14', '15', '1600', '165', '18', '1957', '1975', '1takımdan']\n",
      "Vocabularies 3010 to 3030:\n",
      "['flamingo', 'flamingolar', 'flamingolara', 'flamingoları', 'flamingoların', 'flood', 'floryadan', 'flört', 'fondotensiz', 'football', 'forma', 'formasına', 'formasıyla', 'format', 'forumlar', 'forvet', 'foseptikten', 'foto', 'fotolarım', 'fotolarını']\n"
     ]
    }
   ],
   "source": [
    "eature_names = vect.get_feature_names()\n",
    "\n",
    "print(\"First 20 vocabularies:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Vocabularies 3010 to 3030:\\n{}\".format(feature_names[3010:3030]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer `ngram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 3), preprocessor=None,\n",
       "        stop_words=['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey', 'birşey', 'birkaç', 'nerde', 'nasıl', 'neden', 'sanki', 'gibi', 'ama', 'kim', 'defa', 'de', 'eğer', 'mu', 'her', 'nerede', 'o', 'nereye', 'mü'],\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words=list(stops), ngram_range=(1,3))\n",
    "\n",
    "vect.fit(x_cyber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<2400x632 sparse matrix of type '<class 'numpy.int64'>'\\n\\twith 8236 stored elements in Compressed Sparse Row format>\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = vect.transform(x_cyber)\n",
    "repr(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 vocabularies:\n",
      "['09052018', '0islam', '10', '100', '100m', '1085', '10yıl', '11', '110', '12', '1200', '13', '14', '15', '1600', '165', '18', '1957', '1975', '1takımdan']\n",
      "Vocabularies 3010 to 3030:\n",
      "['flamingo', 'flamingolar', 'flamingolara', 'flamingoları', 'flamingoların', 'flood', 'floryadan', 'flört', 'fondotensiz', 'football', 'forma', 'formasına', 'formasıyla', 'format', 'forumlar', 'forvet', 'foseptikten', 'foto', 'fotolarım', 'fotolarını']\n"
     ]
    }
   ],
   "source": [
    "eature_names = vect.get_feature_names()\n",
    "\n",
    "print(\"First 20 vocabularies:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Vocabularies 3010 to 3030:\\n{}\".format(feature_names[3010:3030]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularies using min_df=1 and n_gram=(1, 1) with highest tfidf: \n",
      "['bam' 'sınav' 'gittin' 'sıffır' 'uzat' 'olmuyor' 'hödük' 'vay' 'tarzdır'\n",
      " 'hee' 'şişko' 'sisko' 'tırrek' 'diyarbakır' 'tatlısın' 'tatlı' 'styling'\n",
      " 'andaval' 'güzelsin' '09052018']\n",
      "The number of vocabularies: 9635\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'mutluyum' 'lan' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=1 and n_gram=(1, 2) with highest tfidf: \n",
      "['angut var' 'mutluyum aq' 'güzelsin çocuk' 'harikasın sen' 'güzelsin kız'\n",
      " 'embesil seni' 'güzelsin güzel' 'uzat' 'ulan amk' 'hee' 'şişko' 'andaval'\n",
      " 'styling' 'tırrek' 'sisko' 'güzelsin' 'tatlı' 'tatlısın' 'diyarbakır'\n",
      " '09052018']\n",
      "The number of vocabularies: 26375\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=1 and n_gram=(1, 3) with highest tfidf: \n",
      "['hee' 'herkes gerizeka' 'angut var' 'mutluyum aq' 'güzelsin çocuk'\n",
      " 'harikasın sen' 'güzelsin kız' 'embesil seni' 'güzelsin güzel' 'ulan amk'\n",
      " 'şişko' 'sisko' 'styling' 'tatlı' 'tatlısın' 'güzelsin' 'diyarbakır'\n",
      " 'tırrek' 'andaval' '09052018']\n",
      "The number of vocabularies: 41386\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'mi' 'sana' 'hayırlı' 'andaval']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=1 and n_gram=(2, 3) with highest tfidf: \n",
      "['şehir yalnizliği' 'bakıyorsun dallama' 'harikasın prenses'\n",
      " 'ağzına sıçarım' 'kinci pezevenk' 'huzurlu sabahlar' 'siktir puşt'\n",
      " 'sanane amıcık' 'titrek serap' 'çookk tatlısın' 'beyinsiz tudor'\n",
      " 'bok boğazlısın' 'sanırım harikasın' 'hanımcı pezevenk' 'embesil seni'\n",
      " 'sapık dinbazlar' 'aşağılık aşifte' 'pornocu olmuş' 'naber berduş'\n",
      " 'sikik insanlarsınız']\n",
      "The number of vocabularies: 31751\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'tam bir' 'kutlu olsun' 'hayırlı olsun' 'orospu çocukları'\n",
      " 'siktir git' 'bir şarkı' 'bir gün' 'hayırlı sabahlar' 'günün kutlu'\n",
      " 'lanet olsun' 'sen kimsin' 'günün kutlu olsun' 'doğum günün kutlu'\n",
      " 'doğum günün' 'bir pislik' 'değil mi' 'hadi hayırlısı' 'hayırlı cumalar'\n",
      " 'amına koyduğum']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=2 and n_gram=(1, 1) with highest tfidf: \n",
      "['bütün' 'zampara' 'demokrasi' 'demek' 'dedin' 'dangalak' 'harikasın'\n",
      " 'sahip' 'zilli' 'dallama' 'konseri' 'cüce' 'zor' 'yarak' 'sanane'\n",
      " 'hayirlisi' 'cenabet' 'hayran' 'bücür' 'değerli']\n",
      "The number of vocabularies: 2330\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'andaval']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=2 and n_gram=(1, 2) with highest tfidf: \n",
      "['sapık' 'bok' 'sanırım' 'günaydın' 'dallama' 'cüce' 'merhaba' 'yaa'\n",
      " 'sahip' 'güzelsin' 'cenabet' 'yolda' 'bütün' 'bücür' 'böyle' 'öküz'\n",
      " 'yarak' 'sanane' 'başarılı' 'harikasın']\n",
      "The number of vocabularies: 2954\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'mi' 'sana' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=2 and n_gram=(1, 3) with highest tfidf: \n",
      "['islama' 'başlasın' 'hocam' 'yolda' 'berduş' 'hikayesi' 'herkesi' 'herkes'\n",
      " 'hazan' 'hayırlısı' 'yosma' 'bilgisayar' 'youtube' 'bir' 'hayırlı' 'veda'\n",
      " 'hayirlisi' 'harikasın' 'güzelsin' 'hayran']\n",
      "The number of vocabularies: 3193\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'andaval']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=2 and n_gram=(2, 3) with highest tfidf: \n",
      "['kahpe evladı' 'kadar saçma' 'kadar orospu' 'kadar mutluyum'\n",
      " 'kadar güzelsin' 'kitap fuarı' 'kadar güzel' 'işin var' 'işi gücü'\n",
      " 'iyi sen' 'iyi kötü' 'iyi bir' 'insanları üzmeyin' 'insanlar var'\n",
      " 'insan kadar' 'insan hakları' 'ilk yarı' 'hıncal uluç' 'kabul etti'\n",
      " 'kadar andaval']\n",
      "The number of vocabularies: 863\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'tam bir' 'kutlu olsun' 'hayırlı olsun' 'orospu çocukları'\n",
      " 'siktir git' 'bir gün' 'bir şarkı' 'hayırlı sabahlar' 'günün kutlu'\n",
      " 'günün kutlu olsun' 'doğum günün' 'doğum günün kutlu' 'lanet olsun'\n",
      " 'sen kimsin' 'bir pislik' 'değil mi' 'amına koyduğum' 'hayırlı cumalar'\n",
      " 'hadi hayırlısı']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=3 and n_gram=(1, 1) with highest tfidf: \n",
      "['müziğin' 'dürzü' 'birde' 'müzik' 'kadar' 'mutluyum' 'bir' 'bilgisayar'\n",
      " 'düğün' 'son' 'sol' 'berduş' 'sokarım' 'hazan' 'ben' 'yarak' 'kahpe'\n",
      " 'kaldı' 'soytarı' 'karşı']\n",
      "The number of vocabularies: 1235\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'mi' 'sana' 'hayırlı' 'andaval']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=3 and n_gram=(1, 2) with highest tfidf: \n",
      "['oluyor' 'cenabet' 'kardeşim' 'herkesi' 'sapık' 'hafta' 'yalaka' 'bücür'\n",
      " 'karşı' 'herkes' 'kaybetmekorkusu' 'soğuk' 'bok' 'keko' 'birde' 'kemal'\n",
      " 'yarak' 'olsun' 'böyle' 'bütün']\n",
      "The number of vocabularies: 1341\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=3 and n_gram=(1, 3) with highest tfidf: \n",
      "['devam' 'hafta' 'kardeşim' 'karşı' 'demokrasi' 'cenabet' 'demek' 'yaa'\n",
      " 'kaybetmekorkusu' 'dangalak' 'yalaka' 'dallama' 'cüce' 'cok' 'herif'\n",
      " 'keko' 'kemal' 'kendi' 'oluyor' 'hayır']\n",
      "The number of vocabularies: 1348\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'mi' 'sana' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=3 and n_gram=(2, 3) with highest tfidf: \n",
      "['hayırlısı olsun' 'hayırlı sabahlar' 'aşağılık pislik' 'hayırlı olsun'\n",
      " 'hayırlı cumalar' 'hayırlı akşamlar' 'hassiktir lan' 'hadi hayırlısı'\n",
      " 'güzelsin be' 'güzel bir' 'aşırı mutluyum' 'başarılar dilerim' 'gün boyu'\n",
      " 'başarılı bir' 'gerizeka tudor' 'geri zekalı' 'gerek yok' 'fatih terim'\n",
      " 'düğün var' 'allah aşkına']\n",
      "The number of vocabularies: 113\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'tam bir' 'kutlu olsun' 'hayırlı olsun' 'orospu çocukları'\n",
      " 'siktir git' 'bir şarkı' 'bir gün' 'günün kutlu' 'hayırlı sabahlar'\n",
      " 'lanet olsun' 'doğum günün kutlu' 'günün kutlu olsun' 'doğum günün'\n",
      " 'sen kimsin' 'bir pislik' 'amına koyduğum' 'değil mi' 'hadi hayırlısı'\n",
      " 'hayırlı cumalar']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=4 and n_gram=(1, 1) with highest tfidf: \n",
      "['sen' 'fahişe' 'seni' 'senin' 'evde' 'et' 'sersem' 'sikerim' 'sikik'\n",
      " 'sinema' 'düğün' 'dürzü' 'sizin' 'dünyanın' 'sokarım' 'sol' 'son' 'sonra'\n",
      " 'pezevenk' 'şırfıntı']\n",
      "The number of vocabularies: 815\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'mutluyum' 'lan' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=4 and n_gram=(1, 2) with highest tfidf: \n",
      "['kaldı' 'tam' 'kahpe' 'kadın' 'kadar' 'izmir' 'türkiye' 'tatlı' 'iyi'\n",
      " 'istiyorum' 'insan' 'tebrikler' 'ilkbahar' 'ibne' 'teşekkür' 'teşekkürler'\n",
      " 'hırsız' 'hödük' 'tatlısın' 'şırfıntı']\n",
      "The number of vocabularies: 863\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'mutluyum' 'lan' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=4 and n_gram=(1, 3) with highest tfidf: \n",
      "['gerizeka' 'gerek' 'sersem' 'düğün' 'geldi' 'sikik' 'gece' 'sinema'\n",
      " 'galatasaray' 'sizin' 'sokarım' 'sol' 'son' 'sonra' 'fena' 'fahişe' 'evde'\n",
      " 'et' 'sikerim' 'şırfıntı']\n",
      "The number of vocabularies: 866\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'mutluyum' 'lan' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'andaval']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=4 and n_gram=(2, 3) with highest tfidf: \n",
      "['belanı versin' 'bi siktir' 'bir adam' 'bir araya' 'bir bok' 'bir gün'\n",
      " 'bir insan' 'hayırlı sabahlar' 'bir orospu' 'bir sürü' 'bir şarkı'\n",
      " 'danla bilic' 'değil mi' 'düğün var' 'gerek yok' 'gerizeka tudor'\n",
      " 'hadi hayırlısı' 'hayırlı akşamlar' 'bir pislik' 'hayırlı cumalar']\n",
      "The number of vocabularies: 51\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'tam bir' 'kutlu olsun' 'hayırlı olsun' 'orospu çocukları'\n",
      " 'siktir git' 'bir gün' 'bir şarkı' 'günün kutlu' 'hayırlı sabahlar'\n",
      " 'günün kutlu olsun' 'doğum günün' 'doğum günün kutlu' 'lanet olsun'\n",
      " 'sen kimsin' 'hadi hayırlısı' 'amına koyduğum' 'hayırlı cumalar'\n",
      " 'değil mi' 'bir pislik']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=5 and n_gram=(1, 1) with highest tfidf: \n",
      "['cüce' 'cok' 'oldu' 'oldum' 'cibiliyetsiz' 'cerrahpaşaduruşu' 'olmasın'\n",
      " 'olmaz' 'olmazsa' 'olmuş' 'olsa' 'olsun' 'cenabet' 'bütün' 'bücür' 'böyle'\n",
      " 'oluyor' 'burada' 'merhaba' 'şırfıntı']\n",
      "The number of vocabularies: 607\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'mi' 'sana' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=5 and n_gram=(1, 2) with highest tfidf: \n",
      "['sanırım' 'harikasın' 'sen' 'hangi' 'sende' 'senden' 'seni' 'senin' 'hala'\n",
      " 'hafta' 'sersem' 'güzelsin' 'sikerim' 'sikik' 'günaydın' 'sinema' 'göt'\n",
      " 'sizin' 'sokarım' 'şırfıntı']\n",
      "The number of vocabularies: 630\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'mutluyum' 'lan' 'pislik' 'gerizeka' 'yok' 'mi' 'sana' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=5 and n_gram=(1, 3) with highest tfidf: \n",
      "['hödük' 'hırsız' 'kitap' 'kerhane' 'kendi' 'keko' 'karşı' 'kardeşim'\n",
      " 'kaldı' 'senin' 'kahpe' 'kadar' 'iyi' 'istiyorum' 'insan' 'imza'\n",
      " 'ilkbahar' 'ibne' 'kadın' 'şırfıntı']\n",
      "The number of vocabularies: 632\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'mi' 'sana' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=5 and n_gram=(2, 3) with highest tfidf: \n",
      "['sen kimsin' 'orospu çocuğu' 'orospu çocukları' 'nefret ediyorum'\n",
      " 'lanet olsun' 'kutlu olsun' 'kadar güzel' 'hayırlı sabahlar'\n",
      " 'hayırlı olsun' 'hadi hayırlısı' 'siktir git' 'gerizeka tudor' 'değil mi'\n",
      " 'danla bilic' 'bir şarkı' 'bir pislik' 'bir orospu' 'bir gün'\n",
      " 'hayırlı cumalar' 'tam bir']\n",
      "The number of vocabularies: 25\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'tam bir' 'kutlu olsun' 'hayırlı olsun' 'orospu çocukları'\n",
      " 'siktir git' 'bir gün' 'bir şarkı' 'günün kutlu' 'hayırlı sabahlar'\n",
      " 'sen kimsin' 'lanet olsun' 'doğum günün' 'doğum günün kutlu'\n",
      " 'günün kutlu olsun' 'amına koyduğum' 'değil mi' 'bir pislik'\n",
      " 'hayırlı cumalar' 'hadi hayırlısı']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=6 and n_gram=(1, 1) with highest tfidf: \n",
      "['bok' 'kız' 'kızlar' 'bizi' 'la' 'lan' 'lazım' 'lütfen' 'makyaj' 'birde'\n",
      " 'bir' 'masturbasyon' 'bilgisayar' 'merhaba' 'bi' 'mi' 'millet' 'beşiktaş'\n",
      " 'mutluyum' 'şırfıntı']\n",
      "The number of vocabularies: 472\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'andaval']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=6 and n_gram=(1, 2) with highest tfidf: \n",
      "['dilerim' 'misin' 'dikkat' 'mutluyum' 'olsun' 'müzik' 'ders' 'demokrasi'\n",
      " 'ol' 'demek' 'dangalak' 'dan' 'oldu' 'oldum' 'dalyarak' 'dallama' 'olmaz'\n",
      " 'olmuş' 'devam' 'şırfıntı']\n",
      "The number of vocabularies: 490\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'mutluyum' 'lan' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=6 and n_gram=(1, 3) with highest tfidf: \n",
      "['la' 'lan' 'böyle' 'bunu' 'lazım' 'lütfen' 'makyaj' 'buna' 'masturbasyon'\n",
      " 'bugün' 'merhaba' 'mi' 'millet' 'bok' 'bizi' 'misin' 'mutluyum' 'müzik'\n",
      " 'sabah' 'şırfıntı']\n",
      "The number of vocabularies: 492\n",
      "Vocabularies with lowest idf:\n",
      "['bir' 'sen' 'ben' 'kadar' 'senin' 'var' 'bi' 'orospu' 'amk' 'seni' 'olsun'\n",
      " 'lan' 'mutluyum' 'pislik' 'gerizeka' 'yok' 'sana' 'mi' 'hayırlı' 'güzel']\n",
      "-----------------------------------\n",
      "Vocabularies using min_df=6 and n_gram=(2, 3) with highest tfidf: \n",
      "['doğum günün' 'doğum günün kutlu' 'günün kutlu' 'günün kutlu olsun'\n",
      " 'amına koyduğum' 'sen kimsin' 'orospu çocuğu' 'orospu çocukları'\n",
      " 'lanet olsun' 'kutlu olsun' 'hayırlı sabahlar' 'hadi hayırlısı'\n",
      " 'hayırlı cumalar' 'siktir git' 'değil mi' 'bir şarkı' 'bir pislik'\n",
      " 'bir gün' 'hayırlı olsun' 'tam bir']\n",
      "The number of vocabularies: 20\n",
      "Vocabularies with lowest idf:\n",
      "['orospu çocuğu' 'tam bir' 'kutlu olsun' 'hayırlı olsun' 'orospu çocukları'\n",
      " 'siktir git' 'bir gün' 'bir şarkı' 'günün kutlu' 'hayırlı sabahlar'\n",
      " 'sen kimsin' 'doğum günün' 'doğum günün kutlu' 'günün kutlu olsun'\n",
      " 'lanet olsun' 'amına koyduğum' 'değil mi' 'bir pislik' 'hayırlı cumalar'\n",
      " 'hadi hayırlısı']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for min_df in [1,2,3,4,5,6]:\n",
    "    for n_gram in [(1,1), (1,2), (1,3), (2,3)]:\n",
    "        \n",
    "        tf = TfidfVectorizer(min_df=min_df, stop_words=stops, ngram_range=n_gram)\n",
    "\n",
    "        x_cyber_train = tf.fit_transform(x_cyber)\n",
    "\n",
    "        max_value = x_cyber_train.max(axis=0).toarray().ravel()\n",
    "\n",
    "        sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "        feature_names = np.array(tf.get_feature_names())\n",
    "\n",
    "        print(\"Vocabularies using min_df={} and n_gram={} with highest tfidf: \\n{}\".format(min_df, n_gram, feature_names[sorted_by_tfidf[-20:]]))\n",
    "\n",
    "        print(\"The number of vocabularies: {}\".format(len(tf.vocabulary_)))\n",
    "\n",
    "        sorted_by_idf = np.argsort(tf.idf_)\n",
    "        print(\"Vocabularies with lowest idf:\\n{}\".format(feature_names[sorted_by_idf[:20]]))\n",
    "        print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = strat_train_set.message\n",
    "y_train = strat_train_set.cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = strat_test_set.message\n",
    "y_test = strat_test_set.cyberbullying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models for `CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svc_pipeline = Pipeline([\n",
    "        ('countvectorizer', CountVectorizer()),\n",
    "        ('linearsvc', LinearSVC(max_iter=2000))\n",
    "])\n",
    "\n",
    "naive_pipeline = Pipeline([\n",
    "        ('countvectorizer', CountVectorizer()),\n",
    "        ('multinomialnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "tree_pipeline = Pipeline([\n",
    "        ('countvectorizer', CountVectorizer()),\n",
    "        ('decisiontreeclassifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "forest_pipeline = Pipeline([\n",
    "        ('countvectorizer', CountVectorizer()),\n",
    "        ('randomforestclassifier', RandomForestClassifier(n_estimators=100))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model_names = [\"Linear SVC\", \"Multinomial Naive Bayes\", \"Decision Tree\", \"Random Forest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n",
      "Best cross-validation score: 89.92\n",
      "Best parameters:  {'countvectorizer__min_df': 1, 'countvectorizer__stop_words': ['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey', 'birşey', 'birkaç', 'nerde', 'nasıl', 'neden', 'sanki', 'gibi', 'ama', 'kim', 'defa', 'de', 'eğer', 'mu', 'her', 'nerede', 'o', 'nereye', 'mü'], 'linearsvc__C': 10, 'countvectorizer__ngram_range': (1, 1)}\n",
      "[[1198    0]\n",
      " [   1 1201]]\n",
      "Recall score of X_train: 99.92%\n",
      "Precision score of X_train: 100.00%\n",
      "F1_score: 99.96%\n",
      "Test score: 88.35%\n",
      "--------------------------\n",
      "Multinomial Naive Bayes\n",
      "Best cross-validation score: 87.38\n",
      "Best parameters:  {'countvectorizer__min_df': 1, 'countvectorizer__stop_words': ['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey', 'birşey', 'birkaç', 'nerde', 'nasıl', 'neden', 'sanki', 'gibi', 'ama', 'kim', 'defa', 'de', 'eğer', 'mu', 'her', 'nerede', 'o', 'nereye', 'mü'], 'countvectorizer__ngram_range': (1, 3)}\n",
      "[[1196    2]\n",
      " [   1 1201]]\n",
      "Recall score of X_train: 99.92%\n",
      "Precision score of X_train: 99.83%\n",
      "F1_score: 99.88%\n",
      "Test score: 87.02%\n",
      "--------------------------\n",
      "Decision Tree\n",
      "Best cross-validation score: 87.21\n",
      "Best parameters:  {'countvectorizer__min_df': 1, 'countvectorizer__stop_words': ['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey', 'birşey', 'birkaç', 'nerde', 'nasıl', 'neden', 'sanki', 'gibi', 'ama', 'kim', 'defa', 'de', 'eğer', 'mu', 'her', 'nerede', 'o', 'nereye', 'mü'], 'countvectorizer__ngram_range': (1, 3)}\n",
      "[[1198    0]\n",
      " [   1 1201]]\n",
      "Recall score of X_train: 99.92%\n",
      "Precision score of X_train: 100.00%\n",
      "F1_score: 99.96%\n",
      "Test score: 87.19%\n",
      "--------------------------\n",
      "Random Forest\n",
      "Best cross-validation score: 88.08\n",
      "Best parameters:  {'countvectorizer__min_df': 3, 'countvectorizer__stop_words': ['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey', 'birşey', 'birkaç', 'nerde', 'nasıl', 'neden', 'sanki', 'gibi', 'ama', 'kim', 'defa', 'de', 'eğer', 'mu', 'her', 'nerede', 'o', 'nereye', 'mü'], 'countvectorizer__ngram_range': (1, 1)}\n",
      "[[1194    4]\n",
      " [  15 1187]]\n",
      "Recall score of X_train: 98.75%\n",
      "Precision score of X_train: 99.66%\n",
      "F1_score: 99.21%\n",
      "Test score: 86.36%\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "param_svc = [ \n",
    "    {\n",
    "        'linearsvc__C': [0.01, 0.1, 1, 10, 100], \n",
    "        'countvectorizer__min_df': [1,3,5], \n",
    "        'countvectorizer__stop_words': [None, list(stops)],\n",
    "        'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n",
    "    } \n",
    "]\n",
    "\n",
    "params_general = [ \n",
    "    {\n",
    "        'countvectorizer__min_df': [1,3,5], \n",
    "        'countvectorizer__stop_words': [None, list(stops)],\n",
    "        'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n",
    "    }\n",
    "]\n",
    "\n",
    "for model, params, name in zip([svc_pipeline, naive_pipeline, tree_pipeline, forest_pipeline],\n",
    "                               [param_svc, params_general, params_general, params_general],\n",
    "                                model_names):\n",
    "\n",
    "    grid = GridSearchCV(model, params, cv=10)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(name)\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_ * 100))\n",
    "    print(\"Best parameters: \", grid.best_params_)\n",
    "    \n",
    "    y_train_pred = grid.predict(X_train)\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "    print(\"Recall score of X_train: {:.2f}%\".format(recall_score(y_train, y_train_pred) * 100))\n",
    "    print(\"Precision score of X_train: {:.2f}%\".format(precision_score(y_train, y_train_pred) * 100))\n",
    "    print(\"F1_score: {:.2f}%\".format(f1_score(y_train, y_train_pred) * 100))\n",
    "    \n",
    "    final_model = grid.best_estimator_\n",
    "    final_test_prediction = final_model.score(X_test, y_test)\n",
    "    print(\"Test score: {:.2f}%\".format(final_test_prediction * 100))    \n",
    "    print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models for `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tfv_pipeline = Pipeline([\n",
    "        ('tfidfvectorizer', TfidfVectorizer()),\n",
    "        ('linearsvc', LinearSVC(max_iter=2000))\n",
    "])\n",
    "\n",
    "naive_tfv_pipeline = Pipeline([\n",
    "        ('tfidfvectorizer', TfidfVectorizer()),\n",
    "        ('multinomialnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "tree_tfv_pipeline = Pipeline([\n",
    "        ('tfidfvectorizer', TfidfVectorizer()),\n",
    "        ('decisiontreeclassifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "forest_tfv_pipeline = Pipeline([\n",
    "        ('tfidfvectorizer', TfidfVectorizer()),\n",
    "        ('randomforestclassifier', RandomForestClassifier(n_estimators=100))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC\n",
      "Best cross-validation score: 89.79\n",
      "Best parameters:  {'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__stop_words': ['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey', 'birşey', 'birkaç', 'nerde', 'nasıl', 'neden', 'sanki', 'gibi', 'ama', 'kim', 'defa', 'de', 'eğer', 'mu', 'her', 'nerede', 'o', 'nereye', 'mü'], 'linearsvc__C': 1, 'tfidfvectorizer__min_df': 1}\n",
      "[[1198    0]\n",
      " [   1 1201]]\n",
      "Recall score of X_train: 99.92%\n",
      "Precision score of X_train: 100.00%\n",
      "F1_score: 99.96%\n",
      "Test score: 88.69%\n",
      "--------------------------\n",
      "Multinomial Naive Bayes\n",
      "Best cross-validation score: 87.96\n",
      "Best parameters:  {'tfidfvectorizer__stop_words': ['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey', 'birşey', 'birkaç', 'nerde', 'nasıl', 'neden', 'sanki', 'gibi', 'ama', 'kim', 'defa', 'de', 'eğer', 'mu', 'her', 'nerede', 'o', 'nereye', 'mü'], 'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__min_df': 1}\n",
      "[[1197    1]\n",
      " [   1 1201]]\n",
      "Recall score of X_train: 99.92%\n",
      "Precision score of X_train: 99.92%\n",
      "F1_score: 99.92%\n",
      "Test score: 87.35%\n",
      "--------------------------\n",
      "Decision Tree\n",
      "Best cross-validation score: 85.58\n",
      "Best parameters:  {'tfidfvectorizer__stop_words': ['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey', 'birşey', 'birkaç', 'nerde', 'nasıl', 'neden', 'sanki', 'gibi', 'ama', 'kim', 'defa', 'de', 'eğer', 'mu', 'her', 'nerede', 'o', 'nereye', 'mü'], 'tfidfvectorizer__ngram_range': (1, 3), 'tfidfvectorizer__min_df': 3}\n",
      "[[1197    1]\n",
      " [  19 1183]]\n",
      "Recall score of X_train: 98.42%\n",
      "Precision score of X_train: 99.92%\n",
      "F1_score: 99.16%\n",
      "Test score: 85.69%\n",
      "--------------------------\n",
      "Random Forest\n",
      "Best cross-validation score: 87.12\n",
      "Best parameters:  {'tfidfvectorizer__stop_words': ['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey', 'birşey', 'birkaç', 'nerde', 'nasıl', 'neden', 'sanki', 'gibi', 'ama', 'kim', 'defa', 'de', 'eğer', 'mu', 'her', 'nerede', 'o', 'nereye', 'mü'], 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__min_df': 3}\n",
      "[[1194    4]\n",
      " [  16 1186]]\n",
      "Recall score of X_train: 98.67%\n",
      "Precision score of X_train: 99.66%\n",
      "F1_score: 99.16%\n",
      "Test score: 86.86%\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "param_svc = [ \n",
    "    {\n",
    "        'linearsvc__C': [0.01, 0.1, 1, 10, 100], \n",
    "        'tfidfvectorizer__min_df': [1,3,5,6], \n",
    "        'tfidfvectorizer__stop_words': [list(stops)],\n",
    "        'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n",
    "    } \n",
    "]\n",
    "\n",
    "params_general = [ \n",
    "    {\n",
    "        'tfidfvectorizer__min_df': [1,3,5,6], \n",
    "        'tfidfvectorizer__stop_words': [list(stops)],\n",
    "        'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)]\n",
    "    }\n",
    "]\n",
    "\n",
    "for model, params, name in zip([svc_tfv_pipeline, naive_tfv_pipeline, tree_tfv_pipeline, forest_tfv_pipeline],\n",
    "                               [param_svc, params_general, params_general, params_general],\n",
    "                                model_names):\n",
    "\n",
    "    grid = GridSearchCV(model, params, cv=10)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(name)\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_ * 100))\n",
    "    print(\"Best parameters: \", grid.best_params_)\n",
    "    \n",
    "    y_train_pred = grid.predict(X_train)\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "\n",
    "    print(\"Recall score of X_train: {:.2f}%\".format(recall_score(y_train, y_train_pred) * 100))\n",
    "    print(\"Precision score of X_train: {:.2f}%\".format(precision_score(y_train, y_train_pred) * 100))\n",
    "    print(\"F1_score: {:.2f}%\".format(f1_score(y_train, y_train_pred) * 100))\n",
    "    \n",
    "    final_model = grid.best_estimator_\n",
    "    final_test_prediction = final_model.score(X_test, y_test)\n",
    "    print(\"Test score: {:.2f}%\".format(final_test_prediction * 100))    \n",
    "    print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick the 'best' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_svc_pipeline = Pipeline([\n",
    "        ('tfidfvectorizer', TfidfVectorizer(min_df=1, ngram_range=(1,2), stop_words=list(stops))),\n",
    "        ('linearsvc', LinearSVC(max_iter=2000, C=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth...ax_iter=2000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'tfidfvectorizer__stop_words': [['çok', 'siz', 'hep', 'ki', 'çünkü', 'ile', 'yani', 'biz', 'diye', 'hem', 'acaba', 'biri', 'niçin', 'bazı', 'mı', 'ne', 'daha', 'az', 'en', 'hepsi', 've', 'tüm', 'ise', 'da', 'kez', 'veya', 'bu', 'hiç', 'ya', 'için', 'niye', 'belki', 'aslında', 'şu', 'şey...ectorizer__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)], 'tfidfvectorizer__min_df': [1, 3, 5, 6]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(naive_svc_pipeline, params, cv=10)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 89.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_ * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Score: 0.886855241265\n",
      "[[258  42]\n",
      " [ 26 275]]\n"
     ]
    }
   ],
   "source": [
    "final_model = grid.best_estimator_\n",
    "final_test_prediction = final_model.score(X_test, y_test)\n",
    "print(\"Final Test Score:\", final_test_prediction)\n",
    "\n",
    "y_test_pred = grid.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_model_svc.pkl']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(final_model, \"final_model_svc.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Score: 0.886855241265\n"
     ]
    }
   ],
   "source": [
    "my_best_model = joblib.load(\"final_model_svc.pkl\")\n",
    "\n",
    "final_test_prediction = my_best_model.score(X_test, y_test)\n",
    "print(\"Final Test Score:\", final_test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_best_model.predict(['gayet iyi', 'konuşma lan', 'bu ne', 'bu ne salak'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
